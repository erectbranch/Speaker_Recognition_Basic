# Section 6: Early Speaker Recognition Approaches

ì´ë²ˆ Sectionì—ì„œëŠ” voice identification ë„ë©”ì¸ì˜ ë”¥ëŸ¬ë‹ ë¶€ìƒ ì „(1995~2013)ì— ì‚¬ìš©ëœ ê¸°ìˆ ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

![history of voice-id techniques](images/history_voice-id.png)

## 6.1 Gaussian Distribution

> [First Principles of Computer Vision youtube: Gaussian Mixture Model | Object Tracking](https://youtu.be/0nz8JMyFF14): ë¹„ì „ ë„ë©”ì¸ì—ì„œì˜ ì‘ìš©

ë°°ê²½ ì§€ì‹ìœ¼ë¡œ **Normal Distribution**(ì •ê·œ ë¶„í¬) í˜¹ì€ **Gaussian Distribution**(ê°€ìš°ì‹œì•ˆ ë¶„í¬)ë¡œ ë¶ˆë¦¬ëŠ” ì—°ì† í™•ë¥  ë¶„í¬ë¥¼ ì‚´í´ë³´ì.

> ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ë‹¨ìˆœí•˜ê³  í”í•  ë¿ë”ëŸ¬, **Central Limit Theorem**(CLT, ì¤‘ì‹¬ê·¹í•œì •ë¦¬)ì— ë”°ë¥´ë©´ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì‰½ê²Œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.

![Gaussian Distribution](images/Gaussian.png)

> ì£¼ë¡œ $x \sim N(x|\mu , {\sigma}^2)$ ë¡œ í‘œí˜„í•œë‹¤.

- $\mu$ : mean

- $\sigma$ : standard deviation

- ${\sigma}^2$ : variance

ê·¸ë ‡ë‹¤ë©´ Cumulative Distribution Function(CDF, ëˆ„ì  ë¶„í¬ í•¨ìˆ˜)ì˜ ë„í•¨ìˆ˜ì— í•´ë‹¹ë˜ëŠ” **Probability Density Function**(PDF, í™•ë¥  ë°€ë„ í•¨ìˆ˜)ëŠ” ì–´ë–»ê²Œ ì •ì˜ë ê¹Œ?

![PDF, CDF](images/PDF_CDF.png)

- Gaussian Distributionì—ì„œì˜ PDFëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.

```math
p(x) = {{1} \over {\sqrt{2 \pi} \sigma}} e^{-{{1} \over {2}}({{x - \mu} \over {\sigma}})^2}
```

---

### 6.1.1 Multivariate Gaussian Distribution

> [ë‹¤ë³€ëŸ‰ í™•ë¥ ë³€ìˆ˜ë€?](https://ilovedata.github.io/teaching/linearmodel-grad/_book/mulivar.html)

> [HashPi: Visualizing a multivariate Gaussian](https://www.hashpi.com/visualizing-a-multivariate-gaussian)

$K$ ì°¨ì›ì˜ í™•ë¥  ë³€ìˆ˜ $x$ ê°€ ìˆì„ ë•Œì˜, **Multivariate Gaussian Distribution**(ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬, ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬)ì„ ì‚´í´ë³´ì.

![multivariate Gaussian Distribution](images/multivariate_Gaussian.png)

> ì£¼ë¡œ $x \sim N(x|\mu, \Sigma)$ ë¡œ í‘œí˜„í•œë‹¤.

- $x$ : $K$ ì°¨ì› random vector

- $\mu$ : $K$ ì°¨ì› mean vector

- $\Sigma$ : $K \times K$ covariance matrix(ê³µë¶„ì‚° í–‰ë ¬)

Probability density functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.

```math
p(x) = {{1} \over {\sqrt{(2 \pi)^K | \Sigma |} }} e^{-{{1} \over {2}}{{(x - \mu)^T} {\Sigma}^{-1}(x-\mu)}}
```

---

### 6.1.2 Why Gaussian Distribution?

ê·¸ë ‡ë‹¤ë©´ speaker recognitionì—ì„œëŠ” ë¬´ì—‡ì„ Gaussian Distributionìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ê¹Œ?

- íŠ¹ì • word/phonemeì˜ Fundamental frequency

- Formants

- Intensity

- Signal-to-noise ratio

- Length of a word/phoneme

---

## 6.2 Gaussian Mixture Model

**Gaussian Mixture Model**(GMM)ì˜ í•µì‹¬ì€, ì–´ë– í•œ complicated distributionì„ $M$ ê°œì˜ Gaussian distributionì˜ weighted sumìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ì•„ì´ë””ì–´ì´ë‹¤.

![GMM example](images/GMM_ex.png)

PDFëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ Gaussian distributionì˜ í•©ì‚°ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

$$ p(x) = \sum_{i=1}^{M} c_i \cdot N_i (x) $$

- $c_i \ge 0$ : $i$ ë²ˆì§¸ Gaussian distributionì˜ weight

- $\sum_{i=1}^{M} c_i = 1$ : weightì˜ ì´í•©ì€ 1ì´ë‹¤.

- Gaussian $N_i (x)$

```math
N_i (x) = {{1} \over {\sqrt{(2 \pi)^K | {\Sigma}_i |} }} e^{-{{1} \over {2}}{{(x - {\mu}_i)^T} {\Sigma}_i^{-1}(x-{\mu}_i)}}
```

ì´ë•Œ ëª¨ë“  íŒ¨ëŸ¬ë¯¸í„°ë¥¼ í¬í•¨í•˜ëŠ” parameter set $\lambda$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.

$$ \lambda =  \lbrace c_i , {\mu}_i , {\Sigma}_i ; i = 1, 2, ..., M \rbrace $$

---

### 6.2.1 Covariance matrix

$\lambda$ ê°€ ê°–ëŠ” ì´ íŒ¨ëŸ¬ë¯¸í„° ê°œìˆ˜ë¥¼ ì•Œì•„ë³´ì.

$$ \lambda =  \lbrace c_i , {\mu}_i , {\Sigma}_i ; i = 1, 2, ..., M \rbrace $$

- $c$ : real number $M$ ê°œ

- ${\mu}$ : $K$ ì°¨ì› vector $M$ ê°œ 

- ${\Sigma}$ : $K \times K$ matrix $M$ ê°œ

- ë”°ë¼ì„œ ì´ \#parametersëŠ” $M \cdot(1+K+K^2)$ ì´ë‹¤. 

ê·¸ëŸ°ë° $K$ ê°€ í¬ë©´ í´ìˆ˜ë¡ covariance matrixê°€ ê°–ëŠ” ì›ì†Œì˜ ê°œìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œ ë˜ê³ , ë”°ë¼ì„œ **overfitting**ì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤. parameter explosionì„ ë§‰ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í…Œí¬ë‹‰ì„ ì‚¬ìš©í•œë‹¤.

- **Sharing** covariance matrix

  - ê° speakerì˜ ëª¨ë“  componentsê°€ ë™ì¼í•œ covariance matrixë¥¼ ê°–ê²Œ í•œë‹¤.

  - ëª¨ë“  speakerì˜ ëª¨ë“  componentsê°€ ë™ì¼í•œ covariance matrixë¥¼ ê°–ê²Œ í•œë‹¤.

- **Simpler** covariance matrix

  - $K \times K$ full covariance matrix ëŒ€ì‹  **diagonal** covariance matrixë¥¼ ì‚¬ìš©í•œë‹¤.

    > ë³´í†µ ê° speakerì˜ ê° componentë§ˆë‹¤ ìì‹ ë§Œì˜ diagonal covariance matrixë¥¼ ê°–ë„ë¡ ì‚¬ìš©í•œë‹¤.

    $\rightarrow$ í–‰ë ¬ì˜ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ê°€ $K$ ê°œë¡œ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.

---

### 6.2.2 Diagonal Covariance Matrix

í•˜ë‚˜ì˜ covariance Gaussianì€ ì—¬ëŸ¬ ê°œì˜ diagonal covariance Gaussianì˜ sumìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![diagonal](images/diagonal_sum.png)

diagonal covariance matrixì˜ ì´ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \lambda =  \lbrace c_i , {\mu}_i , {\sigma}_i ; i = 1, 2, ..., M \rbrace $$

- \#parameters: $M \cdot (1 + 2K)$

ì´ë•Œ distributionì„ í‘œí˜„í•˜ëŠ” ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë°”ë€ë‹¤.

$$ N_i(x) = \prod_{k=1}^{K} {{1} \over {\sqrt{2\pi} {\sigma}_{ik}}}e^{- {{1} \over {2{\sigma}_{ik}^2}}(x_k - {\mu}_{ik})^2} $$

---

## 6.3 GMM for speaker modeling

Speaker Recognitionì—ì„œ GMMì„ ì´ìš©í•´ speaker modelë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![GMM for speaker modeling](images/GMM_for_speaker_modeling.png)

ë¨¼ì € speaker Aì˜ ì—¬ëŸ¬ utteranceì—ì„œ acoustic featureë¥¼ ì¶”ì¶œí•œë‹¤.

  ![feature extraction](images/feature_extraction.png)

  > acoustic features: PLP, MFCC, LFBE ë“±

ë‹¤ìŒì€ GMMì˜ parameter set $\lambda$ ë¥¼ **Maximum Likelihood Estimation**(MLE)ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì •í•œë‹¤.ì´ ê³¼ì •ì„ **parameter estimation**ë¼ê³  í•˜ë©°, ë‹¤ìŒ ì‹ì—ì„œ ê²°ê³¼ë¥¼ maximizeí•˜ëŠ” parameter set $\lambda$ ë¥¼ ì°¾ëŠ”ë‹¤.

$$ p(X|\lambda) = \prod_{n=1}^{N} p(x_n|\lambda) $$

- features $X$

  ë‚´ê°€ ê°€ì§„ ë°ì´í„°ì…‹ì— í•´ë‹¹ëœë‹¤. $X$ ëŠ” ì„œë¡œ independentí•˜ë‹¤.

$$ X = (x_1, x_2, \cdots , x_N) $$

> utteranceì˜ ê¸¸ì´, speaker ìˆ˜, speaker modelì˜ í¬ê¸°(Gaussian component ìˆ˜)ì— ë”°ë¼ ê³„ì‚°ëŸ‰ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ ìˆë‹¤.

---

### 6.3.1 Expectation-Maximization Algorithm

ì´ë•Œ MLEë¥¼ í’€ê¸° ìœ„í•´ì„œ **Expectation-Maximization**(EM) Algorithmì„ ì‚¬ìš©í•œë‹¤.

1. Initialize $\lambda$

   random initialization í˜¹ì€ K-means clusteringì„ ì‚¬ìš©í•˜ì—¬ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ${\lambda}_0$ ë¡œ ì´ˆê¸°í™”í•œë‹¤.

   > ì´ë•Œ ê° clusterëŠ” multivariate Gaussian distributionìœ¼ë¡œ í‘œí˜„ëœë‹¤.

2. E-step

   iteration $t$ ì—ì„œì˜ parameter set ${\lambda}^{t-1}$ ì´ ìˆë‹¤ê³  í•˜ì. ê° sample $x_n$ ê³¼ $i$ ë²ˆì§¸ component ì‚¬ì´ì˜ 'membership values'ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ p(i|x_n,{\lambda}^{(t-1)}) = {{{c}_i^{(t-1)} \cdot N_{i}^{(t-1)}(x_n)} \over {{\sum}_{j=1}^{M}c_j^{(t-1)}\cdot N_{j}^{(t-1)}(x_n)}} $$

3. M-step
 
   - membership valuesë¥¼ ë°”íƒ•ìœ¼ë¡œ, iteration $t$ ì—ì„œì˜ weightsë¥¼ updateí•œë‹¤.
   
   - iteration $t$ ì—ì„œì˜ mean, diagonal covariance matrixë¥¼ updateí•œë‹¤.

$$ c_i^{(t)} = {1 \over N}{\sum}_{n=1}^N p(i|x_n , {\lambda}^{(t-1)}) $$

$$ {\mu}_i^{(t)} = {{\sum_{n=1}^{N}p(i|x_n,{\lambda}^{(t-1)}) \cdot x_n} \over {\sum_{n=1}^{N}p(i|x_n, {\lambda}^{(t-1)})}} $$

$$ ({\sigma}_{ik}^{2})^{(t)} = {{\sum_{n=1}^{N}p(i|x_n,{\lambda}^{(t-1)}) \cdot (x_{nk} - {\mu}_{nk})^2} \over {\sum_{n=1}^{N}p(i|x_n, {\lambda}^{(t-1)})}} $$

4. 2(E-step), 3(M-step)ì„ ë°˜ë³µí•œë‹¤.

   ì¢…ë£ŒëŠ” (1) ì •í•´ë‘” iteration ìµœëŒ€ì¹˜ì— ë„ë‹¬í•˜ê±°ë‚˜, (2) ì´ì „ iterationê³¼ì˜ ì°¨ì´ê°€ ì •í•´ë‘” thresholdë³´ë‹¤ ì‘ì„ ë•Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œë‹¤.

---

### 6.3.2 Singularities

í•˜ì§€ë§Œ parameter estimation ê³¼ì •ì—ì„œ **singularities** ë¬¸ì œë¥¼ ì£¼ì˜í•´ì•¼ í•œë‹¤.

- ${\sigma}_{ik}^2$ ê°€ ë§¤ìš° ì‘ì€ ê°’ì„ ê°–ëŠ”ë‹¤ë©´, Gaussian ê³„ì‚°ì—ì„œ overflowê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ singularitiesë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ í…Œí¬ë‹‰ì„ ì‚¬ìš©í•œë‹¤.

- **Flooring** : ${\sigma}_{ik}^2$ ì˜ ìµœì†Œê°’ì„ ì •í•´ì¤€ë‹¤.

- **Variance Clamping** : ${\sigma}_{ik}^2$ ì˜ ìµœëŒ€ê°’ì„ ì •í•´ì¤€ë‹¤.

---

### 6.3.3 Speaker Identification with GMM

ì´ì²˜ëŸ¼ parameter estimationì„ ê±°ì¹˜ë©´ $S$ ëª…ì˜ candidate speakersë¥¼ GMM modelë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ ê° candidate speakerì˜ GMM paramtersëŠ” ${\lambda}_1, {\lambda}_2, \cdots, {\lambda}_S$ ê°€ ëœë‹¤.

ì´ì œ ìƒˆë¡œìš´ acoustic feature $X$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ featureê°€ ì–´ë–¤ candidate speakerì˜ ê²ƒì¸ì§€ë¥¼ íŒë³„í•´ì•¼ í•œë‹¤. (**speaker identification**)

- $S$ candidate speakerì˜ prior probabilities(ì‚¬ì „ í™•ë¥ )ê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •í•  ë•Œ, best speakerëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ s^{*} = \arg \max_{1 \le s \le S} \ln p(X|{\lambda}_s) $$

$$ \quad \quad = \arg \max_{1 \le s \le S} \sum_{n=1}^{N} \ln p(x_n|{\lambda}_s) $$

---

## 6.4 Universal Background Model

í•˜ì§€ë§Œ ìœ„ì™€ ê°™ì€ ë°©ë²•ì€ **closed set** speaker identificationì—ì„œ ì •ì˜í•œ ê²ƒìœ¼ë¡œ, **open set** speaker identificationì—ì„œëŠ” í™œìš©í•  ìˆ˜ ì—†ë‹¤.

- impostersê°€ ìˆì„ ìˆ˜ ìˆë‹¤.

- Log-likelihoodë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

- ëª¨ë“  speakerì˜ GMMë¥¼ êµ¬í•˜ê¸° ìœ„í•´, ë„ˆë¬´ ë§ì€ parameter estimation ì—°ì‚°ì´ í•„ìš”í•  ìˆ˜ ìˆë‹¤.

  > ë˜í•œ ëª‡ëª‡ speakerì—ê²ŒëŠ” ì¶©ë¶„í•œ dataê°€ ì—†ì„ ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ alternative speakers, impostersë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” speaker-independent GMMì„ ì‚¬ìš©í•œë‹¤. ì´ë¥¼ **Universal Background Model**(UBM)ì´ë¼ê³  ì§€ì¹­í•œë‹¤. UBMì€ ëª¨ë“  speakerê°€ í™œìš©í•  ìˆ˜ ìˆëŠ” ë²”ìš©ì ì¸ ëª¨ë¸ì´ë‹¤.

UBMì„ ì‚¬ìš©í•˜ëŠ” speaker recognition ë¬¸ì œëŠ” **hypothesis testing problem**ë¡œ ì •ì˜í•œë‹¤. ë¨¼ì € ë‹¤ìŒê³¼ ê°™ì´ ë‘ ê°€ì§€ hypothesisë¥¼ ì •ì˜í•´ ë³´ì.

- $H_0$ : speaker $S$ ì˜ utteranceë¡œ ê°€ì •

- $H_1$ : speaker $S$ ê°€ ì•„ë‹Œ ë‹¤ë¥¸ speakerì˜ utteranceë¡œ ê°€ì •

ì´ë•Œ Log-likelihood ratioëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ë‹¤.

$$ L(X) = \ln p(X|\lambda) - \ln p(X|{\lambda}_{H1}) $$

- $\ln p(X|\lambda)$ : speaker $S$ ì˜ GMM

- $\ln p(X|{\lambda}_{H1})$ : GMM-UBM

- íŒë‹¨ì€ ë‹¤ìŒê³¼ ê°™ì´ ì´ë£¨ì–´ì§„ë‹¤.

  - $L(X) > 0$ ì¼ ê²½ìš°: $H_0$ accept

  - otherwise: $H_1$ accept

UBMì€ GMMê³¼ ë§ˆì°¬ê°€ì§€ë¡œ EM algorithmì„ ì‚¬ìš©í•˜ì—¬ parameter estimationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë“  speakerì˜ training dataë¥¼ pooling(í•˜ë‚˜ë¡œ ê²°í•©)í•˜ì—¬ ìˆ˜í–‰í•œë‹¤.

- training ì¤‘ì—ëŠ” imposter dataê°€ ì—†ë‹¤.

- UBMì€ ë°˜ë“œì‹œ **speaker-independent**í•´ì•¼ í•œë‹¤.

---

### 6.4.1 Bayesian Adaptation

GMM-UBM ëª¨ë¸ì„ íšë“í–ˆë‹¤ë©´ individual speakerì˜ GMMì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ë•Œ speakerì˜ GMMì„ ì§ì ‘ ì¶”ì •í•˜ì§€ ì•Šê³ , ëŒ€ì‹  ëª¨ë¸ì˜ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ **Bayesian adaptation**ì„ ì‚¬ìš©í•˜ì—¬ ê° speaker dataì— adaptì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ êµ¬í•œë‹¤.

![Bayesian adaptation](images/Bayesian_adaptation.png)

- ëª¨ë“  GMMì€ ë™ì¼í•œ UBMì„ ì‚¬ìš©í•˜ì—¬ adaptationí•œë‹¤.

- adaptationì€ insufficient dataë§Œìœ¼ë¡œë„ ê°€ëŠ¥í•˜ë‹¤.

---

#### 6.4.1.1 Different adaptation strategies

Bayesian adaptationì„ ì ìš©í•  ë•Œ, weight, means, covariancesì˜ adapt/freeze ì—¬ë¶€ë¥¼ ê²°ì •í•´ì•¼ í•œë‹¤.

- adapt everything(weights, means, covariances)

- adapt weights / freeze means, covariances

- adapt means, covariances / freeze weights

- adapt means / freeze weights, covariances

ì´ ì¤‘ì—ì„œ 4ë²ˆì§¸ê°€ ê°€ì¥ optimalí•œ ë°©ë²•ì´ë‹¤. ë˜í•œ ëª¨ë“  componentsê°€ ê°–ëŠ” meansë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ê²°í•©í•˜ì—¬ í‘œí˜„í•˜ëŠ” ê²½ìš°ê°€ ë§ì€ë°, ì´ëŸ¬í•œ concatenated vectorë¥¼ **supervector**ë¼ê³  í•œë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: supervector ì°¨ì› êµ¬í•˜ê¸° &nbsp;&nbsp;&nbsp;</span>

í•˜ì§€ë§Œ ì£¼ì˜í•  ì ì€ supervectorê°€ ê°–ëŠ” **dimensionality**ê°€ ë„ˆë¬´ í´ ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì˜ˆì‹œì—ì„œ supervectorê°€ ê°–ëŠ” ì°¨ì›ì„ êµ¬í•˜ë¼.

- acoustic feature: 39ì°¨ì› MFCC

- GMM: 512ê°œ components

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

supervectorê°€ ê°–ëŠ” ì°¨ì›ì€ ë‹¹ë¯€ê³¼ ê°™ë‹¤.

$$39 \times 512 = 19,968$$

ë˜í•œ supervectorëŠ” speakerì˜ ë‹¤ë¥¸ information, ì˜ˆë¥¼ ë“¤ì–´ microphone, acoustic environment(room reverberation ë“±), background noise ë“±ì„ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•œë‹¤.

---

### 6.4.2 training workflow

GMM-UBMì„ ì´ìš©í•œ speaker modelì˜ training workflowì„ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![GMM-UBM workflow](images/GMM-UBM_workflow.png)

1. speaker-independentí•œ UBMì„ í›ˆë ¨í•œë‹¤.

2. ê° speakerì˜ feature dataë¥¼ ê°€ì§€ê³ , Bayesian adaptationì„ í†µí•´ ê° speakerì˜ GMMì„ êµ¬í•œë‹¤.

---

## 6.5 Support Vector Machine(SVM)

**Support Vector Machine**(SVM)ì€ classification, regression ë“± ë§ì€ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ê¸°ê³„í•™ìŠµ ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. íŠ¹íˆ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ë¶€ìƒ ì „, speaker recognitionì—ì„œ GMM-SVMì´ êµ‰ì¥íˆ ë§ì´ ì‚¬ìš©ë˜ì—ˆë‹¤.

SVMì€ **maximum-margin** principleì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ binary classification ë°ì´í„°ì…‹ì´ ìˆë‹¤ê³  í•˜ì.

$$ \lbrace(x_1, y_1), ..., (x_N, y_N)\rbrace $$

- $x_i$ : feature vector

- $y_i \in \lbrace -1, 1 \rbrace$ ; binary ground truth label

ëª©í‘œëŠ” $y_i = 1$ , $y_i = -1$ ì„ êµ¬ë¶„í•˜ëŠ” hyperplane $w \cdot x - b$ ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ë™ì‹œì— marginì€ ìµœëŒ€í™”í•´ì•¼ í•œë‹¤.

![SVM](images/SVM.png)

> íŒŒë€ìƒ‰ ì : data = 1 / ì´ˆë¡ìƒ‰ ì : data = -1 / ì§ì„ : hyperplane

- hyperplaneì— ìœ„ì¹˜í•œ ì ì„ **support vector**ë¼ê³  ì§€ì¹­í•œë‹¤.

- margin : ${{2} \over {||w||}}$

  ë”°ë¼ì„œ marginì„ ìµœëŒ€í™”í•˜ë ¤ë©´ norm $||w||$ ë¥¼ ìµœì†Œí™”í•´ì•¼ í•œë‹¤.

  > ì´ë•Œ lineê³¼ ê°€ì¥ ê°€ê¹Œìš´ íŒŒë€ìƒ‰ ì , ì´ˆë¡ìƒ‰ ì ê³¼ì˜ ê±°ë¦¬ëŠ” ì¼ì¹˜í•œë‹¤.

- constraintëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ y_i \cdot (w \cdot x_i - b) \ge 1 $$

---

### 6.5.2 Linear SVM with soft margin

í•˜ì§€ë§Œ ì–¸ì œë‚˜ ë‘ classê°€ linearly seperableí•˜ë‹¤ëŠ” ë³´ì¥ì€ ì—†ë‹¤. ë”°ë¼ì„œ ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¥¼ í—ˆìš©í•˜ëŠ” **soft-margin**ì„ ë„ì…í•œë‹¤. ì´ë•Œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ê°€ **hinge loss**ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ë‘ ê°€ì§€ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

- classification error (hinge loss)

- margin size

ë‹¤ìŒ soft marginì„ ì‚¬ìš©í•˜ëŠ” linear SVM ì˜ˆì‹œ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.

![SVM with soft-margin](images/SVM_soft-margin.png)

- hinge loss

$$ \max(0, 1 - y_i \cdot (w \cdot x_i - b)) $$

- soft-margin SVM

  - $\lambda$ : ë‘ targets ì‚¬ì´ì˜ tradeoffë¥¼ ì¡°ì ˆí•˜ëŠ” íŒ¨ëŸ¬ë¯¸í„°

$$ {\min}_{w,b} \left ( \left ({1 \over N}\sum_{i=1}^{N}\max(0, 1 - y_i \cdot (w \cdot x_i - b))\right) + \lambda || w ||^2 \right) $$

ì´ë•Œ ë¬¸ì œë¥¼ $w$ ì™€ $b$ ë¥¼ ìµœì í™”í•˜ëŠ” ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ìƒˆë¡œìš´ ë°ì´í„° $x$ ë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´ì„œ scoring functionì„ ê³„ì‚°í•´ ë‚˜ê°„ë‹¤.

$$ g(x) = w \cdot x - b $$

---

## 6.6 Non-linear SVM

í•˜ì§€ë§Œ ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” linear classificationì„ ì ìš©í•  ë§Œí¼ feature spaceê°€ ì´ìƒì ì¸ ê²½ìš°ëŠ” ì ë‹¤. ë”°ë¼ì„œ ë¨¼ì € feature spaceì— ì ìš©í•  **non-linear transform** $\phi (\cdot)$ ì„ ì°¾ì•„ì•¼ í•œë‹¤. Non-linear SVMì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

- ëª¨ë“  feature $x$ ë¥¼ $\phi (x)$ ë¡œ ë³€í™˜í•œë‹¤.

- $\phi (x)$ ì— linear SVMì„ ì ìš©í•œë‹¤.

---

### 6.6.1 Kernel trick

**kernel trick**ì„ ì´ìš©í•˜ë©´ non-linear transform $\phi (\cdot)$ ë¥¼ ì°¾ì„ í•„ìš” ì—†ì´, **kernel function**ë§Œì„ ì •ì˜í•´ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 

- kernel function

$$ K(x_1, x_2) = \phi (x_1)^T \phi (x_2) $$

- scoring function

  - $\alpha$ : ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œ 0ì´ë©°, support vectorì— í•´ë‹¹í•˜ëŠ” ê²½ìš°ì—ë§Œ 0ì´ ì•„ë‹Œ ê°’ì„ ê°–ëŠ”ë‹¤. (training ê³¼ì •ì„ í†µí•´ êµ¬í•œë‹¤.)

$$ g(x) = \left( \sum_{i=1}^{N} {\alpha}_i y_i K(x, x_i) \right) + b $$

---

### 6.6.2 Commonly used kernel functions

ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” kernel functionsìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¢…ë¥˜ê°€ ìˆë‹¤.

- Degree-d polynomial kernel

$$ K(x_1, x_2) = (x_1 \cdot x_2 + 1)^d $$

- Degree-d homogeneous polynomial kernel

$$ K(x_1, x_2) = (x_1  \cdot x_2)^d $$

- Radial Basis Function(RBF) kernel $( \gamma > 0)$

  > ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” kernel functionì´ë‹¤.

$$ K(x_1, x_2) = \exp(- \gamma ||x_1 - x_2||^2) $$

- Hyperbolic tangent kernel ( $\kappa > 0, c < 0$ )

$$ K(x_1, x_2) = \tanh (-\kappa x_1 \cdot x_2 + c) $$

---

## 6.7 Kernel functions for speaker recognition

### 6.7.1 GMM supervector linera kernel

ì•ì„œ GMM-UBMì—ì„œ êµ¬í•œ supervectorë¥¼ ì‚¬ìš©í•˜ì—¬, kernel functionìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ linear kernelì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$ K(U_a, U_b) = \sum_{i=1}^{M} c_i ({\mu}_i^a)^T {\Sigma}_i^{-1} {\mu}_i^b $$

- $\lbrace {\mu}_i^{a} \rbrace$ , $\lbrace {\mu}_i^{b} \rbrace$  : ë‘ utterance $U_a$ , $U_b$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ ê° adapted means

- $\lbrace c_i \rbrace$ , $\lbrace {\Sigma}_i \rbrace$ : UVMì˜ weights, covariances

---

### 6.7.2 GMM L2 inner product kernel

GMM supervector linear kernelë³´ë‹¤ ì„±ëŠ¥ì€ ì¢‹ì§€ ì•Šìœ¼ë‚˜ ë‹¤ìŒê³¼ ê°™ì´ kernel functionì„ ì •ì˜í•  ìˆ˜ë„ ìˆë‹¤.

> original GMM-UBMë³´ë‹¤ëŠ” í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ëŠ”ë‹¤.

$$ K(U_a, U_b) = \sum_{i=1}^{M} \sum_{j=1}^{M} c_i c_j N({\mu}_i^a - {\mu}_j^b | 0, {\Sigma}_i + {\Sigma}_j) $$

---

## 6.8 Workflow of GMM-SVM

GMM-SVMì˜ workflowë¥¼ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. ë¨¼ì € UBMì„ êµ¬í•œë‹¤.

2. ê° <U>utterance</U>ë¥¼ ê°€ì§€ê³  adaptì‹œí‚¨ë‹¤.

3. kernel functionì„ ì‚¬ìš©í•´ (speakerë¥¼ ë¶„ë¥˜í•˜ëŠ”) non-linear SVM classifierë¥¼ í•™ìŠµí•œë‹¤.

ì—¬ê¸°ì„œ GMM-UBMê³¼ ê°€ì¥ í° ì°¨ì´ì ì€ ë‘ ê°€ì§€ë¥¼ ë“¤ ìˆ˜ ìˆë‹¤.

- GMMì„ speakerê°€ ì•„ë‹Œ utteranceë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.

- ê° speakerëŠ” SVM classifierë¥¼ í†µí•´ êµ¬ë¶„ëœë‹¤.

---
